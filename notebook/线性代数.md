# 第一课  线性代数的直观理解--几何与线代的关系

几何的理解：能帮助我们更好的判断出解决特定问题需要什么样的工具、解决过程及解读结果

比如：在线性代数中，**翻转、转置、逆矩阵、降低维度、投射、缩放scale**，等其实都是矩阵的工具、操作行为。

数值的理解：能帮助我们顺利应用这些工具

# 第二课 向量是什么

##一、三种理解：

1、物理：有方向，有长度,**空间中的箭头**

2、计算：有序的列表，向量=列表，向量常常以原点（0， 0）为起点

$\begin{bmatrix} -2 \\ 3\\ \end{bmatrix}$ 

3、数学：两者的结合，只要能满足向量的**加法和乘法**，就行。

##二、线性代数的术语

原点：$\begin{bmatrix} 0 \\ 0 \\ \end{bmatrix}$ 

向量：$\begin{bmatrix} x \\ y \\  \end{bmatrix}$，在x轴走了多远，y轴上走了多远，+ - 表示走的方向。竖着写是惯用方法，也被称为垂直向量，横着写被称为水平向量

维度：$\begin{bmatrix} x^{(1)} \\ \vdots \\ x^{(m)} \\ \end{bmatrix}$ 为m维度，有m个轴

##***三、基本运算 加 乘***

加法：$\vec{w}$ 的起点移动到 $\vec {v}$ 的终点并重合，然后把$\vec{v}$ 的起点和 $\vec{w}$ 的终点连接起来。就是走路

$\begin{bmatrix} x_1 \\ y_1 \\ \end{bmatrix} + \begin{bmatrix} x_2 \\ y_2 \\ \end{bmatrix} = \begin{bmatrix} x_1 + y_1 \\ x_2 + y_2 \\ \end{bmatrix} ​$

向量数乘：就是向量的**缩放scaling**，把$\vec{v}$ 拉长（整数）、缩放（分数）、转向（负数）

​                   用于缩放的数字，被称为**标量 scalar**，这是数字在线性代数中的重要作用

$ 2 \cdot \begin{bmatrix} x \\ y \\ \end{bmatrix} = \begin{bmatrix} 2x \\ 2y \\ \end{bmatrix}$



# 第三课 向量的线性组合 combination、张成空间span和基base

##一、坐标系的基向量：

x轴上的长度为1，方向为正的向量，$\hat{i}=\begin{bmatrix} 1 \\ 0\\ \end{bmatrix}$ 。y轴上的长度为1，方向为正的向量，$\hat{j} = \begin{bmatrix} 0 \\ 1\\ \end{bmatrix}$ 。也可以分别称他们为 x 或 y 方向的单位向量。

基向量base：可以理解为是要操作的**单位向量**，也就是任何向量都可以表示$\begin{bmatrix} x \\ y\\ \end{bmatrix} = \theta_1 \cdot \hat{v} + \theta_2 \hat{w} $

##二、向量的线性组合：

数乘向量的和被称为这些向量的的线性组合，即$\begin{bmatrix} x\\ y\\ \end{bmatrix}$ 就是一个线性组合。这样我们就把数字和向量联系在了一起。

##三、张成空间：

某一基向量在标量取值范围内所得到的所有向量，也就是线性组合的所有结果。共线时，张成空间是一条直线。

为什么点可以表示向量？

因为将起点规定为原点，那么任意一个标量坐标确定的点和原点就能得到一条直线，有长度有方向。

这样单个向量可以看做是直线，多个向量可以看做是点

三维空间的向量张成空间？

只有两个向量，那么得到一个过原点的平面。两个向量所有组合后的终点所构成的**平面**。

三个向量的张成空间：

1、共平面，还是一个平面（降维）

2、不共平面，是一个空间

##四、线性关系

线性相关：一组向量中至少有一个是多余的，没有对张成空间做出任何贡献。也就是，一个向量可以表示为其他向量的线性组合。

线性无关：所有向量都给张成空间增添了新的维度

# 第四课 矩阵与线性变换

##一、线性变换 linear transformation ： 

变换transformation实际上就是函数，将输入变为输出

线性变换linear transformation:保持网格线平行且等距分布

1、直线在变换后仍然保持直线不能弯曲 

2、原点保持固定

## 二、矩阵

矩阵是坐标系基向量变换后的坐标，也描述了基向量的变化。

矩阵是线性变换，是向量的变化

用数值表示线性变换，就是记录$\hat{i}$ 和$\hat{j}$ 的变换。那么任意给一个向量，与坐标基向量的变换进行线性组合，就能得出变换后的向量。

所以坐标系的基向量就是变换，即$\hat{i} \cdot \begin{bmatrix} a && b\\ c && d\\ \end{bmatrix} =1 \cdot\begin{bmatrix} a\\ c\\ \end{bmatrix} + 0$ 所以基向量变为了 $\hat{i}_t=\begin{bmatrix} a\\ c\\ \end{bmatrix}$ 和$\hat{j}_t=\begin{bmatrix} b \\ d\\ \end{bmatrix}$ ，然后对基向量进行缩放。

旋转 rotation

剪切 shear 错切

# 第五课 矩阵乘法与线性变换复合

矩阵代表了一个特定的线性变化。矩阵与向量相乘就是将线性变化作用于向量

$\begin{bmatrix} a  b\\ c d\\ \end{bmatrix} \begin{bmatrix} x \\ y \\ \end{bmatrix}=x\begin{bmatrix} a\\ c\\ \end{bmatrix} + y\begin{bmatrix} b \\ c\\ \end{bmatrix} = \begin{bmatrix} ax+by \\ cx+dy \\ \end{bmatrix}$

##1、复合线性变换

一个变换之后再进行另一个变化

先旋转再剪切，其实只是一个变换。也就是，分步骤的变换可以等同于一个新的变换。

旋转：$\begin{bmatrix}0&-1 \\1& 0 \\ \end{bmatrix}$ * 剪切：$\begin{bmatrix}1& 1 \\ 0&1 \\\end{bmatrix}$ =新的变化：$\begin{bmatrix} 1&-1 \\1&0\\ \end{bmatrix}$

##2、矩阵乘法

计算方法：$\begin{bmatrix} a & c \\ b & d\\ \end{bmatrix}\begin{bmatrix} e &g \\ f&h\\ \end{bmatrix}=\begin{bmatrix}ae+cf & ag+ch \\ be+df & bg+dh\end{bmatrix}$

**注意：乘积从右往左 读，从右往左计算** 类似于 $g(f(x))$ 先算 f 再算 g

其实就是对base的变化

不满足乘法交换律

满足结合律

## 3、附注三维空间的线性变化



#第六课 行列式

## 1、行列式定义

行列式determinant：线性变换改变面积的比例，被称为这个变换的行列式。变换就是矩阵

空间定向改变时，行列式为负值。**为“负”表示平面反转了，所以绝对值表示改变的面积**

计算公式：

$det(\begin{bmatrix} a&c\\ b&d \\ \end{bmatrix})=ad-bc$

##2、三维空间行列式

三维空间行列式为0，表示矩阵的列线性相关。因为肯定有个向量在另外两个向量形成的平面里，没有为向量的张成做贡献，即没有体积。**降维**

右手法则

## 3、定律

分配律：$det(M_1M_2)=det(M_1)det(M_2)$



# 第七课 逆矩阵、列空间与零空间

无计算公式！！！要啥公式，电脑是干啥吃的？！？！？？！

矩阵的用途：

1、描述空间的操纵：计算机图形学 机器人

2、求解特定方程组：线性方程组 **机器学习里的正规方程**

##1、逆矩阵 Inverse matrices

如何解线性方程组：

因为 $ax+by+cz=d\implies\begin{bmatrix}a&b&c\\ \end{bmatrix}\begin{bmatrix}x \\ y\\ z\\ \end{bmatrix}=\begin{bmatrix}d\\\end{bmatrix}\implies A\vec{x}=\vec{v}$ 矩阵A是一个线性变换，所以，$\vec{x}=\vec{v}A^{-1}$，所以通过求A矩阵的逆运算就能得到解

$A^{-1}$成为A的逆

$AA^{-1}=1$ A与A的逆相乘意味着线性变换的抵消，即什么都不做

$det(A)\neq0\implies A^{-1}存在$ 也就是说A的线性变化不是降维，有解

$det(A)=0$ 降维 A不存在A的逆，但依然可以求出来

## 2、秩 Rank

秩：列空间的维数。线性变换后的空间维度。

满秩：空间的最大维度

## 3、列空间 Column space

列空间：张成空间的集合

 满秩=列空间

非满秩意味着，列空间的被压缩了

## 4、零空间 Null space

零空间：变换后落在原点的向量结合，也称为核Kernel



# 第八课 点积

##1、点积公式

$\begin{bmatrix}a\\b\\\end{bmatrix}\cdot\begin{bmatrix}c\\d\\\end{bmatrix}=a\ast c + b\ast d$ 结果是一个标量

几何解释：投影长度与向量长度的乘积。

是将向量投影到某条直线的线性变换。

是将向量投影到单位向量所在直线上所得的投影长度

## 2、降维

矩阵的行数代表维度

矩阵：$\begin{bmatrix}c& d\\\end{bmatrix}$ 横着的称为矩阵1*2的矩阵

向量： $\begin{bmatrix}a \\ b\\\end{bmatrix}$  竖着的称为向量

点积可以看作是 矩阵与向量的乘积:$\begin{bmatrix}a\\b\\\end{bmatrix}\cdot\begin{bmatrix}c\\d\\\end{bmatrix}=\begin{bmatrix}c&d\\\end{bmatrix}\begin{bmatrix}a \\ b\\\end{bmatrix}= a\ast c + b\ast d$

##2、点积与投影的关系

对偶性

点积是投影的工具。

线代就是对向量的操作，通过矩阵操作



#第九课 叉积 

叉积：两个向量组成的四边形的面积。真实定义：同故宫两个三维向量生成一个新的三维向量。

叉积的计算公式：就是基向量变换后的两个向量组成的矩阵的行列式

$\begin{bmatrix}a \\b \\\end{bmatrix}\times\begin{bmatrix}c\\d\\\end{bmatrix}=det(\begin{bmatrix}a & c\\b&d\\\end{bmatrix})=ad-bc$

叉积公式$\begin{bmatrix}v_1\\v_2\\v_3\\\end{bmatrix}\times\begin{bmatrix}w_1\\w_2\\w_3\\\end{bmatrix} = det(\begin{bmatrix}\hat{i}&v_1&w_1\\\hat{j}&v_2&w_2\\\hat{k}&v_3&w_3\\\end{bmatrix})=\hat{i}(v_2w_3-v_3w_2)+\hat{j}(v_3w_1-v_1w_3)+\hat{k}(v_1w_2-v_2w_1)$

$\begin{bmatrix}v_1\\v_2\\v_3\\\end{bmatrix}\times\begin{bmatrix}w_1\\w_2\\w_3\\\end{bmatrix} = \begin{bmatrix}v_2w_3-w_2v_3\\v_3w_1-w_3v_1\\v_1w_2-w_1v_2\\\end{bmatrix}$

**不同矩阵运算的总结**

矩阵乘法：是复合线性变换，矩阵

矩阵点积：是投影操作，是一个标量。降维

矩阵行列式：矩阵线性变换的面积，标量

矩阵叉积：是一个三维向量。垂直于已有两个向量组成的平面的向量，方向符合右手向量。长度为平面的面积。升维