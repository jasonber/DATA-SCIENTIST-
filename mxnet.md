#样式迁移
1.我们首先挑选一个卷积神经网络来提取特征。我们选择它的特定层来匹配样式，特定层来匹配内容。示意图中我们选择层1,2,4作为样式层，层3作为内容层。
2.输入样式图片并保存样式层输出，记第 i 层输出为 si
3.输入内容图片并保存内容层输出，记第 i 层输出为 ci
4.初始化合成图片 x 为随机值或者其他更好的初始值。然后进行迭代使得用 x 抽取的特征能够匹配上 si 和 ci。具体来说，我们如下迭代直到收敛。
5.输入 x 计算样式层和内容层输出，记第 i 层输出为 yi
6.使用样式损失函数来计算 yi 和 si 的差异
7.使用内容损失函数来计算 yi 和 ci 的差异
8.对损失求和并对输入 x 求导，记导数为 g
9.更新 x， 例如 x=x−ηg

内容损失函数使用通常回归用的均方误差。对于样式，我们可以将它看成是像素点在每个通道的统计分布。

匹配统计分布常用的做法是冲量匹配，就是说使得他们有一样的均值，协方差，和其他高维的冲量。为了计算简单起见，我们这里假设卷积输出已经是均值为0了，而且我们只匹配协方差。也就是说，样式损失函数就是对 si 和 yi 计算 Gram 矩阵然后应用均方误差

styleloss(si,yi)=1c2hw‖sisTi−yiyTi‖F
这里假设我们已经将 si 和 yi 变形成了 c×hw 的2D矩阵了。

训练过程跟之前的主要的主要不同在于

这里我们的损失函数更加复杂。
我们只对输入进行更新，这个意味着我们需要对输入x预先分配了梯度。
我们可能会替换匹配内容和样式的层，和调整他们之间的权重，来得到不同风格的输出。这里我们对梯度做了一般化，使得不同参数下的学习率不需要太大变化。
仍然使用简单的梯度下降，但每n次迭代我们会减小一次学习率
